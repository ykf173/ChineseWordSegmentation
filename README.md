# ChineseWordSegmentation
该项目中介绍了jieba,hanlp,snownlp,nlpir,pkuseg,thulac几个常用分词工具，并结合其分词性能与时间效率做了比较

* 实验流程
> 分词工具安装
> [数据下载](http://sighan.cs.uchicago.edu/bakeoff2005/)
> 实验

* 分词工具安装及使用

> 1、[jieba](https://github.com/fxsjy/jieba)
```python
pip3 install jieba 
```
> 2、[hanlp](https://github.com/hankcs/HanLP)
```python
pip3 install hanlp
```
> 3、[pkuseg](https://github.com/lancopku/PKUSeg-python)
```python
pip3 install pkuseg
```
> 4、[thulac](https://github.com/thunlp/THULAC-Python)
```python
pip3 install thulac
```
> 5、[snownlp](https://github.com/isnowfy/snownlp)
```python
pip3 install snownlp
```
> 6、[nlpir](https://github.com/NLPIR-team/NLPIR)
```python
pip3 install pynlpir
```
